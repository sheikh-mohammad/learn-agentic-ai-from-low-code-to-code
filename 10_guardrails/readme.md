# Session 10: Guardrails & Governance (No‑Code)

This session teaches you to build AI systems that are not just functional, but safe, ethical, and properly governed—essential for production deployment and responsible AI development.

## Why
Safety and trust enable deployment. Use the Guardrails Wizard to add checks and approvals.

## Objectives
- Configure moderation, jailbreak, PII masking checks
- Add human approvals and document policy choices
- Re‑test flows and confirm acceptable false‑positive rates

## Lab
1. Enable 2–3 guardrail checks; set thresholds
2. Add approval on high‑risk branch; document criteria
3. Re‑run evals; compare failure modes and user experience


## The Safety and Ethics Imperative

### Why Advanced Safety Matters

**The Reality:** AI systems can cause harm through:
- **Data Exposure:** Leaking sensitive information
- **Harmful Advice:** Providing dangerous or incorrect guidance
- **Bias and Discrimination:** Perpetuating unfair treatment
- **Misinformation:** Spreading false or misleading information
- **Security Vulnerabilities:** Creating attack vectors

**The Solution:** Multi-layer safety systems that prevent harm before it occurs.

### The Ethics Challenge

**Key Questions:**
- How do we ensure AI treats all users fairly?
- When should AI defer to human judgment?
- How transparent should AI systems be?
- Who is responsible for AI decisions?
- How do we balance innovation with safety?

---

## Deliverable
- Guardrails policy doc + screenshots and a short risk rationale